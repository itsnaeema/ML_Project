{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3a872FQpJ+EsOX1yXJDmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsnaeema/ML_Project/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTKyu5KSzhbW"
      },
      "outputs": [],
      "source": [
        "import nltk #(Natural Lnaguage Tool Kit)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(nltk)#we can known all the details about the packages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6KodD_N0Sj3",
        "outputId": "1efd36d3-e6a6-49dc-8b36-6ecf4e9cb0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on package nltk:\n",
            "\n",
            "NAME\n",
            "    nltk\n",
            "\n",
            "DESCRIPTION\n",
            "    The Natural Language Toolkit (NLTK) is an open source Python library\n",
            "    for Natural Language Processing.  A free online book is available.\n",
            "    (If you use the library for academic research, please cite the book.)\n",
            "    \n",
            "    Steven Bird, Ewan Klein, and Edward Loper (2009).\n",
            "    Natural Language Processing with Python.  O'Reilly Media Inc.\n",
            "    https://www.nltk.org/book/\n",
            "    \n",
            "    isort:skip_file\n",
            "    \n",
            "    @version: 3.8.1\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    app (package)\n",
            "    book\n",
            "    ccg (package)\n",
            "    chat (package)\n",
            "    chunk (package)\n",
            "    classify (package)\n",
            "    cli\n",
            "    cluster (package)\n",
            "    collections\n",
            "    collocations\n",
            "    compat\n",
            "    corpus (package)\n",
            "    data\n",
            "    decorators\n",
            "    downloader\n",
            "    draw (package)\n",
            "    featstruct\n",
            "    grammar\n",
            "    help\n",
            "    inference (package)\n",
            "    internals\n",
            "    jsontags\n",
            "    langnames\n",
            "    lazyimport\n",
            "    lm (package)\n",
            "    metrics (package)\n",
            "    misc (package)\n",
            "    parse (package)\n",
            "    probability\n",
            "    sem (package)\n",
            "    sentiment (package)\n",
            "    stem (package)\n",
            "    tag (package)\n",
            "    tbl (package)\n",
            "    test (package)\n",
            "    text\n",
            "    tgrep\n",
            "    tokenize (package)\n",
            "    toolbox\n",
            "    translate (package)\n",
            "    tree (package)\n",
            "    treeprettyprinter\n",
            "    treetransforms\n",
            "    twitter (package)\n",
            "    util\n",
            "    wsd\n",
            "\n",
            "SUBMODULES\n",
            "    agreement\n",
            "    aline\n",
            "    api\n",
            "    arlstem\n",
            "    arlstem2\n",
            "    association\n",
            "    bleu_score\n",
            "    bllip\n",
            "    boxer\n",
            "    brill\n",
            "    brill_trainer\n",
            "    casual\n",
            "    chart\n",
            "    chrf_score\n",
            "    cistem\n",
            "    confusionmatrix\n",
            "    corenlp\n",
            "    crf\n",
            "    decisiontree\n",
            "    dependencygraph\n",
            "    destructive\n",
            "    discourse\n",
            "    distance\n",
            "    drt\n",
            "    earleychart\n",
            "    evaluate\n",
            "    featurechart\n",
            "    gale_church\n",
            "    gdfa\n",
            "    gleu_score\n",
            "    glue\n",
            "    hmm\n",
            "    hunpos\n",
            "    ibm1\n",
            "    ibm2\n",
            "    ibm3\n",
            "    ibm4\n",
            "    ibm5\n",
            "    ibm_model\n",
            "    isri\n",
            "    lancaster\n",
            "    legality_principle\n",
            "    lfg\n",
            "    linearlogic\n",
            "    logic\n",
            "    mace\n",
            "    malt\n",
            "    mapping\n",
            "    maxent\n",
            "    megam\n",
            "    meteor_score\n",
            "    mwe\n",
            "    naivebayes\n",
            "    nist_score\n",
            "    nonprojectivedependencyparser\n",
            "    paice\n",
            "    pchart\n",
            "    perceptron\n",
            "    phrase_based\n",
            "    porter\n",
            "    positivenaivebayes\n",
            "    projectivedependencyparser\n",
            "    prover9\n",
            "    punkt\n",
            "    recursivedescent\n",
            "    regexp\n",
            "    relextract\n",
            "    repp\n",
            "    resolution\n",
            "    ribes_score\n",
            "    rslp\n",
            "    rte_classify\n",
            "    scikitlearn\n",
            "    scores\n",
            "    segmentation\n",
            "    senna\n",
            "    sequential\n",
            "    sexpr\n",
            "    shiftreduce\n",
            "    simple\n",
            "    snowball\n",
            "    sonority_sequencing\n",
            "    spearman\n",
            "    stack_decoder\n",
            "    stanford\n",
            "    stanford_segmenter\n",
            "    tableau\n",
            "    tadm\n",
            "    textcat\n",
            "    texttiling\n",
            "    tnt\n",
            "    toktok\n",
            "    transitionparser\n",
            "    treebank\n",
            "    viterbi\n",
            "    weka\n",
            "    wordnet\n",
            "\n",
            "FUNCTIONS\n",
            "    demo()\n",
            "        # FIXME:  override any accidentally imported demo, see https://github.com/nltk/nltk/issues/2116\n",
            "    \n",
            "    tee(iterable, n=2, /)\n",
            "        Returns a tuple of n independent iterators.\n",
            "\n",
            "DATA\n",
            "    RUS_PICKLE = 'taggers/averaged_perceptron_tagger_ru/averaged_perceptro...\n",
            "    SLASH = *slash*\n",
            "    TYPE = *type*\n",
            "    __author_email__ = 'nltk.team@gmail.com'\n",
            "    __classifiers__ = ['Development Status :: 5 - Production/Stable', 'Int...\n",
            "    __copyright__ = 'Copyright (C) 2001-2023 NLTK Project.\\n\\nDistribut......\n",
            "    __keywords__ = ['NLP', 'CL', 'natural language processing', 'computati...\n",
            "    __license__ = 'Apache License, Version 2.0'\n",
            "    __longdescr__ = 'The Natural Language Toolkit (NLTK) is a Python ...NL...\n",
            "    __maintainer__ = 'NLTK Team'\n",
            "    __maintainer_email__ = 'nltk.team@gmail.com'\n",
            "    __url__ = 'https://www.nltk.org/'\n",
            "    app = <LazyModule 'nltk.app'>\n",
            "    chat = <LazyModule 'nltk.chat'>\n",
            "    corpus = <LazyModule 'nltk.corpus'>\n",
            "    infile = <_io.TextIOWrapper name='/usr/local/lib/python3....packages/n...\n",
            "    json_tags = {'!nltk.tag.BrillTagger': <class 'nltk.tag.brill.BrillTagg...\n",
            "    toolbox = <LazyModule 'nltk.toolbox'>\n",
            "    version_file = '/usr/local/lib/python3.10/dist-packages/nltk/VERSION'\n",
            "\n",
            "VERSION\n",
            "    3.8.1\n",
            "\n",
            "AUTHOR\n",
            "    NLTK Team\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.10/dist-packages/nltk/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing steps\n",
        "#1.remove stopwords(is,or,and,the....)\n",
        "#towork this code we should download the package of stopwoeds\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "sw=stopwords.words('english')#creationg an object to stop word nd passing the language to remove\n",
        "print(len(sw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM6in8Ty0dPt",
        "outputId": "63d0a2b3-cc7c-46ed-b302-8a8d96cc253e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization\n",
        "#dwnld the package of tokenization\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentence='''The Natural Language Toolkit (NLTK) is an open source Python library\n",
        "    for Natural Language Processing.  A free online book is available.'''#for multiple line we use 3 '''\n",
        "words=word_tokenize(sentence)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io570-o312a2",
        "outputId": "69f69a58-c552-4ca1-da1e-c98c6e7e704c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Natural', 'Language', 'Toolkit', '(', 'NLTK', ')', 'is', 'an', 'open', 'source', 'Python', 'library', 'for', 'Natural', 'Language', 'Processing', '.', 'A', 'free', 'online', 'book', 'is', 'available', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#while writting a fresh code we should check and remove all stop words\n",
        "#writing stopword code and tokenization code together\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "sw=stopwords.words('english')\n",
        "sentence='''The Natural Language Toolkit (NLTK) is an open source Python library\n",
        "    for Natural Language Processing.  A free online book is available.'''\n",
        "tokens=word_tokenize(sentence)\n",
        "tokens1=[i.lower() for i in tokens]#by doing all the stop words remove from tokens but The,A not remove bcs of capitl lttr,so we should convert them into small lttr\n",
        "stop_data=[i for i in tokens1 if i not in sw]\n",
        "print(stop_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDPvxzYS45P-",
        "outputId": "664cf43d-5a56-4e83-d13d-c1cdba8ddc58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'toolkit', '(', 'nltk', ')', 'open', 'source', 'python', 'library', 'natural', 'language', 'processing', '.', 'free', 'online', 'book', 'available', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#n-gram\n",
        "# we can also do tokenization using n-gram\n",
        "txt='this is a very book to study'\n",
        "from nltk.util import ngrams\n",
        "data=ngrams(sequence=word_tokenize(txt),n=2)\n",
        "for i in data:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ3T4tP_8bBT",
        "outputId": "c1864662-9ada-40b9-da71-efc262ae7ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('this', 'is')\n",
            "('is', 'a')\n",
            "('a', 'very')\n",
            "('very', 'book')\n",
            "('book', 'to')\n",
            "('to', 'study')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming-cut the tail of word to get root word\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "words=['programm','programming','walking','running','jumping','talking','reached','dancing']\n",
        "for i in words:#i\n",
        "  print(i,\":\",ps.stem(i))#eg-reached:reach\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDqDl9eC9fMN",
        "outputId": "85b6f5a4-7aee-4d2d-e5a2-77c8a2a6c389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "programm : programm\n",
            "programming : program\n",
            "walking : walk\n",
            "running : run\n",
            "jumping : jump\n",
            "talking : talk\n",
            "reached : reach\n",
            "dancing : danc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#snowballstemmer()- another type of stemming\n",
        "from nltk.stem import SnowballStemmer\n",
        "ss=SnowballStemmer('english')\n",
        "for i in words:\n",
        "  print(i,\":\",ss.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzMKzB9n-gPM",
        "outputId": "791fa6c8-bcf4-4c72-e0d9-8cb0910de371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "programm : programm\n",
            "programming : program\n",
            "walking : walk\n",
            "running : run\n",
            "jumping : jump\n",
            "talking : talk\n",
            "reached : reach\n",
            "dancing : danc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatization-Also like stemming work based on algo\n",
        "#in lemmatization all word will not take,also will not cut ing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw=1.4')\n",
        "lem=WordNetLemmatizer()\n",
        "for i in words:\n",
        "  print(i,\":\",lem.lemmatize(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZVPV4xY--n3",
        "outputId": "5b93001f-2e85-4377-e969-6e84424d2fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Error loading omw=1.4: Package 'omw=1.4' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "programm : programm\n",
            "programming : programming\n",
            "walking : walking\n",
            "running : running\n",
            "jumping : jumping\n",
            "talking : talking\n",
            "reached : reached\n",
            "dancing : dancing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to remove a special charcter\n",
        "#using 're :regular expression\n",
        "import re\n",
        "str1='Wonderfull_Peacock@123*4 #%!!!'\n",
        "str2=re.sub('[^A-Za-z0-9]',' ',str1)\n",
        "#str2=re.sub('[A-Z]',' ',str1)# onderfull_ eacock,(replaced by space here),if we use ^ sumpol(ath ozhike baaki ellam replace cheyyum)\n",
        "print(str2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SUD87MlAO6c",
        "outputId": "db0fbe06-dd8d-4d42-f90c-5bf6569d228b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wonderfull Peacock 123 4      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A7gMEr2JEdFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}